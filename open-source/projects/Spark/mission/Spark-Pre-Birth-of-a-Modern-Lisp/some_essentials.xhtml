<?xml version="1.0" encoding="UTF-8"?><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>Some Spark Essentials</title><link rel="stylesheet" type="text/css" href="docbook.css"/><link rel="stylesheet" type="text/css" href="style.css"/><meta name="generator" content="DocBook XSL Shlomif"/><link rel="prev" href="index.xhtml" title="Spark - Pre-Birth of a Modern Lisp"/><link rel="next" href="why_lisp_unpopular.xhtml" title="Why Lisp Has Been Unpopular"/></head><body><header><div class="center ads_top"/><div class="navheader"><table style="width:100%"><tr><th style="text-align:center" colspan="3">Some Spark Essentials</th></tr><tr><td style="width:20%;text-align:left"><a accesskey="p" href="index.xhtml">Prev</a> </td><th style="width:60%;text-align:center"> </th><td style="width:20%;text-align:right"> <a accesskey="n" href="why_lisp_unpopular.xhtml">Next</a></td></tr></table><hr/></div></header><section class="section"><div class="titlepage"><div><div><h2 id="some_essentials" class="title">Some Spark Essentials</h2></div></div></div><div class="toc"><ul class="toc"><li><span class="section"><a href="some_essentials.xhtml#not_another_scheme">Spark is not another implementation of Scheme (or Common Lisp)</a></span></li><li><span class="section"><a href="some_essentials.xhtml#dynamic_lang">Spark will be a dynamic (so-called “scripting”) programming language</a></span></li><li><span class="section"><a href="some_essentials.xhtml#popular">Spark aims to be popular and be actively used for real-world tasks</a></span></li><li><span class="section"><a href="some_essentials.xhtml#not_compete_with_c">Spark does not aim to compete with C and friends</a></span></li><li><span class="section"><a href="some_essentials.xhtml#types">Spark will have a rich type system but won’t be strongly typed</a></span></li><li><span class="section"><a href="some_essentials.xhtml#useful_for_scripting">Spark will be capable of being used for Scripting</a></span><ul><li><span class="section"><a href="some_essentials.xhtml#line_count_script">Line Count:</a></span></li><li><span class="section"><a href="some_essentials.xhtml#line_count_script_reloaded">Line Count Reloaded</a></span></li><li><span class="section"><a href="some_essentials.xhtml#double_space_script">Double-space a file</a></span></li><li><span class="section"><a href="some_essentials.xhtml#number_lines_script">Number lines in each file</a></span></li><li><span class="section"><a href="some_essentials.xhtml#note_cmd_line_magic">Note about command line magic</a></span></li></ul></li><li><span class="section"><a href="some_essentials.xhtml#nested_namespaces">Spark will have nested namespaces</a></span></li><li><span class="section"><a href="some_essentials.xhtml#succinct">Spark will be more succinct than most Lisps, but not overly terse</a></span></li><li><span class="section"><a href="some_essentials.xhtml#written_in_plaintext">Spark will be written in plaintext</a></span></li><li><span class="section"><a href="some_essentials.xhtml#dedicated_syntax_for_re">Regexps and other important elements have dedicated syntax</a></span></li><li><span class="section"><a href="some_essentials.xhtml#c_conventions">C/Perl/etc. conventions</a></span></li><li><span class="section"><a href="some_essentials.xhtml#proliferation_implementations">Spark will not encourage a proliferation of implementations</a></span></li><li><span class="section"><a href="some_essentials.xhtml#ultimate_lisp">The first version of Spark will not be the ultimate Lisp</a></span></li></ul></div><p>These are some of the guiding elements in the design of Spark.</p><section class="section"><div class="titlepage"><div><div><h3 id="not_another_scheme" class="title">Spark is not another implementation of Scheme (or Common Lisp)</h3></div></div></div><p>There are far too many implementations of Scheme out there, and probably too many of Common Lisp. However, that is beside the point that we did not come to praise the existing dialects of Lisp, by implementing them again.</p><p>Spark will be a completely different dialect of Lisp. It won’t be compatible with either Scheme (or any of its implementations), Common Lisp or even with Arc. It will still be Lisp, though, as we hope you will see.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="dynamic_lang" class="title">Spark will be a dynamic (so-called “scripting”) programming language</h3></div></div></div><p>Spark will be an alternative to such languages as Perl, Python, Ruby, PHP, Tcl, Lua or io-language, which are all very different but have APIs to achieve similar ends and are used for similar tasks. Like them and Lisp (which is one of the oldest dynamic languages) it will be able to determine a lot of behaviour at run time and will support dynamic “eval”, call-by-name, run-time typing, run-time change of type for a datum, multiple dispatch, polymorphic macros and other features of Lisp and other dynamic languages.</p><p>Some people have been referring to Perl and friends as "scripting languages" but that implies they are only useful for scripts. See:</p><p><a class="link" href="http://xoa.petdance.com/Stop_saying_%22script%22">http://xoa.petdance.com/Stop_saying_%22script%22</a></p><p>While that article by Andy Lester illustrates the problem with labelling programming languages as “scripting languages”, I still think saying “script” and “scripting” is a valid way to distinguish a trivial program from an application. For example <code class="literal">/usr/bin/gcc</code> is essentially a script written in C, which just passes controls to the various compilation stages. When we type <code class="literal">gcc</code> at the command line, we are running this script that does the hard work of doing the compilation for us. (<code class="literal">/usr/bin/gcc</code> should not be confused with GCC, the GNU Compiler Collection which is a compilation framework for C, C++ and other languages, and is a crucial piece of the open-source UNIX-like operating systems infrastructure).</p><p>For an “in your face” anti-thesis to the aversion to call languages scripting languages see Larry Wall’s “Programming is Hard. Let’s Go Scripting”:</p><p><a class="link" href="http://www.perl.com/pub/a/2007/12/06/soto-11.html">http://www.perl.com/pub/a/2007/12/06/soto-11.html</a></p></section><section class="section"><div class="titlepage"><div><div><h3 id="popular" class="title">Spark aims to be popular and be actively used for real-world tasks</h3></div></div></div><p>While other general purpose Lisps such as Common Lisp, Scheme, Arc or Clojure have been influential and have some followers and users, none of them are actively used with the same popularity as Perl, Python, Ruby or PHP are. Spark aims to be a popular lisp dialect which will be actively used for real-world tasks, not just toy or experimentation code.</p><p>Eventually, it is our hope that some people will get paid to maintain Spark code. Some of them against their best preferences, like some people now are maintaining Perl 5, PHP or even Python code while preferring a different language. (Simply because it puts bread on their table, and they cannot get paid to write something else.)</p><p>Python? Yes! Google for example has been hiring several Perl 5 or even Perl 5 and Perl 6 programmers and instructing them to program mostly in Python. I’ve known two or three of them myself. Many of them tolerate or even like Python enough to program in it, but many of them still prefer Perl and consider themselves as Perlers at heart. But Google gives very good conditions and many Perl programmers (despite a huge demand for them) are treated badly or have advanced to better paying positions. And naturally a lot of the “younger generation” (most of them) bought the hype that you shouldn’t need to learn Perl if you already know PHP, or Python or Java or whatever.</p><p>In any case, Spark aims to become roughly as popular as any of Perl/PHP/etc. It is not guaranteed that we succeed , but we still would like to try. What can we do to increase our chances?</p><p>Paul Graham has written about <a class="link" href="http://www.paulgraham.com/popular.html">what makes a language popular</a>. Arguably, he failed to make Arc as popular as he wanted, even after it was released (and may have violated some of his own guidelines), but the guidelines in the article are still sound (ad-hominem anecdote put aside).</p><p>One can spend a lot of time expanding on Graham’s points and adding more stuff (including possibly good timing and luck and possibly a lot of financial backing can help, which most open-source enthusiasts don’t have).</p><p>However, it is important to note that this is our ultimate goal. Some languages didn’t have a lot of financial backing before they got popular - Perl, Python, PHP and Ruby are examples of it. Can we do the same?</p><p>None of the P-languages are perfect. Someone on #scheme gave me a moderate list of must have items out of which Perl, Python, PHP and Ruby each violate at least one and Scheme and Common Lisp none. I can quote him here as the channel is publicly logged.</p><p>I’m not going to try to violate any of them (or any of the many common sense “never-do-it”’s that PHP alone has violated, to say nothing of the rest) on purpose in hope it will save me from the evil eye of having a decent language. But I don’t think languages succeed because they are bad but because they offer at least one awesome advantage. For example Perl 1 offered the ability to write pseudo-shell scripts in a much more robsut and powerful way than awk, sed, sh and csh and a much easier way than C did. (Although still very primitive by the standards of Perl 5.). By the time Perl 4 was abandoned in favour of Perl 5 , Perl 4 was already the de-facto standard for clueful sys admins who valued their sanity. And I recall when Perl 5 was the only real solution for writing CGI scripts that you could use because I was hired to work as a web developer at that time.</p><p>PHP was similar enough to Perl 5 to fool people into believing it was adequate (while messing up the internal behaviour and core language royally) and offered more ubiquity as far as setting up on web-hosts was concerned (which Perl 5 is still trying to catch up). You can now download very powerful and gratis content management systems in PHP (and other gratis web applications) and run them out of the box on your cheap hosting and call yourself a webmaster, which would be hard with any other UNIX-based solution. PHP as a language has many issues, but they got some parts of the implementation needs right, and this is what made it popular.</p><p>We need to find some good reasons for people to become hooked on Spark. Spark probably won’t be perfect because I am not perfect and don’t know all there is to know about language design. However, neither are Scheme and Common Lisp. But even if Spark doesn’t have so many quirks like PHP or Perl (or even Python) it should be an attractive platform for people to experiment with, write scripts in daily, write toy programs and serious applications, and then one day we may get a killer app like Ruby’s Ruby on Rails.</p><p>(Hopefully, we won’t get the added RoR hype, which I admit is based on viral marketing and community efforts, but was hype nonetheless. Perl and PHP were never strongly hyped, and they seem to have been doing fine without it.)</p></section><section class="section"><div class="titlepage"><div><div><h3 id="not_compete_with_c" class="title">Spark does not aim to compete with C and friends</h3></div></div></div><p>Spark does not aim to fill the same ecological niche as C, C++ Objective-C, etc. - much less assembly. C and friends have been the de-facto standard for writing applications when some or all of certain constraints have been met:</p><p><a class="link" href="http://www.shlomifish.org/philosophy/computers/when-c-is-best/">http://www.shlomifish.org/philosophy/computers/when-c-is-best/</a></p><p>While their use have lately been diminishing somewhat due to the increasing attractiveness of the Java or .NET frameworks or the various dynamic languages (Perl/Python/PHP/Ruby/etc.) they are nevertheless still very much in vogue and even the backends for the more high-level virtual machines are written in C and C++ .</p><p>There have been some efforts to compete with C and C++ on their own turf such as <a class="link" href="http://en.wikipedia.org/wiki/D_%28programming_language%29">D</a> or <a class="link" href="http://www.ecere.com/">Ecere</a> (and earlier efforts such as Ada 95, or Object Pascal) and they can be commended for that, but unlike them, Spark does not aim to replace C in most of the valid use cases for a C-like language.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="types" class="title">Spark will have a rich type system but won’t be strongly typed</h3></div></div></div><p>Like Common Lisp, Python, Ruby and Perl 6 and to some extent unlike Perl 5, Spark will have a rich type system. However, it won’t be strongly typed like Haskell. If Spark had been going to be strongly typed, it could no longer have been considered a Lisp, and I happen to like dynamic typing.</p><p>A variable can be assigned different values with different types during its run-time, and functions would be able to accept variables of any type (unless they specifically forbid it).</p><p>The Spark type system will be extendable at run time, and will be analogous to its Object Oriented Programming (OOP) system. As a result, one would be able to call methods on pieces of data, on expressions, on S-expressions, and on functions, macros, classes and method declarations, and on their application.</p><p>In Spark “everything will be an object”, but unlike Java, it won’t be overly-OO. One won’t need to instantiate a class and declare a method just to print “Hello World” on the screen. This will work:</p><pre class="screen">$ spark -e '(say "Hello World")'</pre><p>Or:</p><pre class="screen">$ spark -e '(print "Hello World\n")'</pre><p>Or:</p><pre class="screen">$ spark -e '(-&gt; "Hello World" say)'</pre><p><code class="literal">→</code> Is a simple macro that converts <code class="literal">(→ obj method args)</code> to <code class="literal">(method obj args)</code> and is used for some syntactic sugar.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="useful_for_scripting" class="title">Spark will be capable of being used for Scripting</h3></div></div></div><div class="toc"><ul class="toc"><li><span class="section"><a href="some_essentials.xhtml#line_count_script">Line Count:</a></span></li><li><span class="section"><a href="some_essentials.xhtml#line_count_script_reloaded">Line Count Reloaded</a></span></li><li><span class="section"><a href="some_essentials.xhtml#double_space_script">Double-space a file</a></span></li><li><span class="section"><a href="some_essentials.xhtml#number_lines_script">Number lines in each file</a></span></li><li><span class="section"><a href="some_essentials.xhtml#note_cmd_line_magic">Note about command line magic</a></span></li></ul></div><p>While Spark should not be called a scripting language, just as the name is a misnomer for Perl 5 or for PHP, it should in fact be capable of writing scripts, including command line scripts at the prompt or the REPL (Read-Eval-Print-Loop). Here are some examples for command line scripts. Most of these are taken from the descriptions in <a class="link" href="http://www.catonmat.net/blog/awk-one-liners-explained-part-one/">Peteris Krumins’ "Famous Awk One-Liners Explained"</a> series (which is now in the process of being augmented with “Famous Perl One-Liners Explained”). I’m not going to study the Awk implementations due to lack of knowledge in Awk and lack of will to learn it as I already know Perl 5 - its far superior superset, but I’ll implement something similar in Spark (Hopefully, Peteris will feature a “Famous Spark One Liners Explained” feature in his blog someday too).</p><section class="section"><div class="titlepage"><div><div><h4 id="line_count_script" class="title">Line Count:</h4></div></div></div><pre class="screen">$ spark -e '(foreach (fh ARGV) (++ i)) (say i)' [Files]</pre></section><section class="section"><div class="titlepage"><div><div><h4 id="line_count_script_reloaded" class="title">Line Count Reloaded</h4></div></div></div><pre class="screen">$ spark -e '(: say len getlines fh ARGV)' [Files]</pre><p><code class="literal">(: …​ )</code> serves the same purpose as Haskell’s <code class="literal">$</code> - to chain function calls without too many nested parameters. So this script is equivalent to saying:</p><pre class="screen">$ spark -e '(say (len (getlines (fh ARGV))))' [Files]</pre><p>But is shorter and cleaner.</p></section><section class="section"><div class="titlepage"><div><div><h4 id="double_space_script" class="title">Double-space a file</h4></div></div></div><pre class="screen">$ spark -pe '(say)'</pre><p>(Think Perl)</p></section><section class="section"><div class="titlepage"><div><div><h4 id="number_lines_script" class="title">Number lines in each file</h4></div></div></div><pre class="screen">$ spark -ne '(say "${^LINENUM} ${^LINE}")'</pre><p>Here we can see the string interpolation of variables in action. <code class="literal">${…​.}</code> interpolates a single variable, while <code class="literal">$()</code> is an S-expression. Aside from that spark will also have sprintf, <a class="link" href="http://metacpan.org/release/Text-Sprintf-Named">sprintf with named conversions similar to Python</a> and something as similar as possible to Perl’s Template Toolkit (while still being Sparky). I find Common Lisp’s <code class="literal">format</code> to be hard to understand and much less flexible than Template Toolkit so I’m going to drop it.</p><p>Like in Perl 5 the <code class="literal">^VARNAME</code> variables are reserved and are usually in all-capitals. Unlike Perl 5 (or Common Lisp), we are not a Lisp-2 and we use the same symbol namespace for everything (like Scheme). So we can put assign a lambda (could be <code class="literal">(fun …​ )</code> , <code class="literal">(sub …​ )</code> , <code class="literal">(lambda …​)</code> or <code class="literal">(function …​ )</code> - all exact synonyms) to a variable and call it with a value:</p><pre class="screen">(my square)
(:= square (fun (x) (* x x))) ; Or (&lt;- square) but I’m hazy about (= square)
(say (square 5)) ; Prints 25 followed by a newline.</pre><p>Like in Perl 5, however, a symbol table has an arbitrary amount of slots which we can put values. So we can say:</p><pre class="screen">(say :to (fh STDERR) "Warning, Will Robinson.")</pre><p>Which will print to STDERR.</p><p>In the case of the <code class="literal">:to</code> named parameter to <code class="literal">(say)</code> (or to <code class="literal">(print)</code> or to <code class="literal">(printf)</code> or whatever we have or define) we don’t need the explicit <code class="literal">(fh …​)</code> (= file handle) call but it won’t hurt.</p></section><section class="section"><div class="titlepage"><div><div><h4 id="note_cmd_line_magic" class="title">Note about command line magic</h4></div></div></div><p>Out of convenience the <code class="literal">-e</code> and the rest of the <code class="literal">-p</code>, <code class="literal">-n</code> , etc. flags will involve some magic manipulation of the S-expression inside -e or inside the script. It also loads some convenient modules. However, sometimes we may wish to convert a command line script to a full application. That’s what the <code class="literal">--dump-code=code.spark</code> flag is for. It dumps the code of the program to a file containing code that can be run with just <code class="literal">spark code.spark</code>.</p><p>For example:</p><pre class="screen">$ spark --dump-code=say.spark -pe '(say)'
$ cat say.spark
(no strict) ; you should probably remove that.
(use re)
(use cmd-loop)
(cmd-loop.set-implicit-print 1)
(say)
$</pre><p>Like all examples here, this is just for the sake of the illustration. Until version 1.0.0 comes out, everything can change. But the concepts will remain the same.</p><p>We encourage Perl, Ruby and other dynamic languages with rich command line interfaces, to steal the <code class="literal">--dump-code</code> idea. Maybe one day someone will become a multi-millionaire from selling a 300K Lines program that evolved from a simple <code class="literal">spark/perl/ruby --dump-code=code…​. -e '…​.\'</code> invocation (after a successful plain <code class="literal">-e</code> invocation).</p></section></section><section class="section"><div class="titlepage"><div><div><h3 id="nested_namespaces" class="title">Spark will have nested namespaces</h3></div></div></div><p>Spark will have a similar namespace system to Perl 5, with nested namespaces, and the ability to selectively import symbols from namespaces at run-time. Similarly to <a class="link" href="http://metacpan.org/release/Sub-Exporter">http://metacpan.org/release/Sub-Exporter</a> and as opposed to C++ where importing symbols from namespaces is an all-or-nothing operation, and so mostly unusable.</p><p>As opposed to Java, one would be able to import several symbols from a namespace at once and grouped by tags. <code class="literal">Sub-Exporter</code> gives much more than that for Perl 5, but I don’t recall all the details from the slides I saw about it. <code class="literal">:-)</code> .</p><p>Like Perl 5 one will be able to import symbols at run-time.</p><p>As opposed to Perl 5, classes won’t be automatically associated with namespaces, and a namespace may contain one or more classes (or none). Like CPAN and unlike Java (<code class="literal">org.apache.jakarta…​</code>), we will not enforce namespace purity, but hopefully there will be a better mechanism than the current CPAN and PAUSE (Perl Authors Upload Server) to be able to fork, spin-off, or branch CSAN distributions or choose between competing alternatives. <a class="link" href="http://cpan6.org/">CPAN6</a> (orthogonal to Perl 6) is worth a look for some ideas, as is <a class="link" href="http://www.cpan.org/misc/ZCAN.html">"The Zen of Comprehensive Archive Networks"</a></p></section><section class="section"><div class="titlepage"><div><div><h3 id="succinct" class="title">Spark will be more succinct than most Lisps, but not overly terse</h3></div></div></div><p>One thing notable about Scheme and Common Lisp is that many identifiers and keywords there are excessively long. <code class="literal">(string→integer)</code> , <code class="literal">(lambda)</code> , <code class="literal">(concatenate)</code> , etc. Yes, you can easily assign aliases to them, but it:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Takes time to write and more time to maintain.</li><li class="listitem">Will require you to carry and update a meta-syntactic package in all your code.</li><li class="listitem">Would make your code harder to understand to the unfamiliar.</li></ol></div><p>So Spark will provide short identifiers for most common operations (either macros or functions) by default. There won’t be too many long aliases because they will increase the core and require more time to learn and become familiar with for the uninitiated, but some of them will be considered.</p><p>For example, a variation on my favourite Scheme statement is:</p><pre class="screen">(vector-set! myarray idx (+ (vector-ref myarray idx) 2))</pre><p>Which in Perl is:</p><pre class="screen">$myarray[$idx] += 2;</pre><p>And in Spark would be:</p><pre class="screen">(+= (myarray idx) 2)</pre><p>Which isn’t much worse than Perl. We’re not trying to beat Perl 5, Perl 6 or much less J in code Golfing in every case - this isn’t a peeing contest. We’re just trying to make easy tasks easy.</p><p>Furthermore, while <span class="strong"><strong>+=</strong></span>, <span class="strong"><strong>-=</strong></span>, <span class="strong"><strong>/=</strong></span> and friends will be defined there will be a macro (that will be used defined by the prelude and used there) to define your own +${op}=+ operators. Like <code class="literal">(list=)</code> or <code class="literal">(myfunc=)</code>.</p><p>And as you see we will try very hard that every sane expression can become an lvalue.</p><p>And in case, you’re worried there will be a “+\+” and “--” operators too, with post-increment/post-decrement and pre-increment/pre-decrement variations.</p><p>Finally, by inspiration from Arc, we decided to do something about excessive parens. So we will have a <code class="literal">(with (k1 v1 k2 v2 k3 v3) …​. )</code> scoping instead of the unwieldy Scheme <code class="literal">(let*)</code> and <code class="literal">(letrec)</code> (both will be easily replaceable by +(with…​)<code class="literal">with some macro or VM trickery.). And we’ll have a C-style for-loop instead of the obscure +(do…​)</code> and a while loop, and a Perl 5/Perl 6-style foreach loop, and maybe other loops too. And you can always use recursion.</p><p>However we’re not going down the Arc route of assigning extremely short, and hard to pronounce and grep for identifiers. <code class="literal">(fn)</code> How do you pronounce it? fnn…​. There’s no such sound in Hebrew, so it’s verboten by your Hebrew-speaking overlords. We like <code class="literal">(fun…​)</code> because it puts the fun back in function (“Functional!! Parallelism!!!!” - oh wait! Wrong language.), and we like <code class="literal">(sub …​)</code> because it puts the “sub” back in subroutine. And all Hebrew speakers will rejoice because they can pronounce <code class="literal">“cat”</code> exactly like <code class="literal">“cut”</code> and Perl like Pérl and Lisp like Leesp, and they can pronounce TeX and LaTeX with a honest-to-god <a class="link" href="http://en.wikipedia.org/wiki/Kaph">khaph</a> (or a <a class="link" href="http://en.wikipedia.org/wiki/Heth">Heth</a> if they put their mind to to it.), and the god of the Israelite programmers saw there were only 5 and a half vowels and he was pleased.</p><p>Seriously now, I don’t like (fn) because it’s hard to pronounce, doesn’t sound right when you read it to your mind’s ear, and is obnoxious. While being succinct is a noble goal, picking psychologically-sound and intuitive conventions is also important. I recall searching the Arc tutorial and documentation for a <code class="literal">(not)</code> function only to found it was spelled <code class="literal">(no)</code>:</p><pre class="screen">(if (no soup) (print "soup is false"))</pre><p>“If no soup”. Oh no, no! No soup for you. <span class="strong"><strong>For one year!!!</strong></span> We’re going have (not) like everybody else, and also a “!” alias .</p><pre class="screen">(if (not soup) (print "soup is false"))</pre><pre class="screen">(if (! soup) (print "soup is false"))</pre><p>So we’re going to borrow stuff from Arc, but only when it makes sense. Spark should have an up-to-date documentation manual right from the very start, which will be kept as up-to-date as possible, and naturally will have automated tests, which would serve as automatically verifiable examples. Arc really had none, and often you needed to read the implementation code, or one of the example web applications.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="written_in_plaintext" class="title">Spark will be written in plaintext</h3></div></div></div><p>What do I mean? <code class="literal">sbcl</code> forces to write a long command line just to run a program from the command line and exit. Arc was not better and could not execute a program directly. I added this functionality to the Arc git repository myself.</p><p>No, the REPL won’t be gone in spark, and good old <code class="literal">(load)</code> will still be there. But you can also do:</p><pre class="screen">$ cat hello.spark
(say "Hello World!")
$ spark hello.spark
Hello World!
$</pre><p>Or put /usr/bin/env spark or whatever in your sha-bang and it will work.</p><p>A spark program will do its thing, execute to the finish and gracefully exit.</p><p>We will still have a REPL, that can be used from the command-line or from within Emacs SLIME or within IDEs such as Eclipse. Even Vim/gvim may get a REPL if it gets something like an embedded shell like Emacs has.</p><p>Spark programs can be abstract syntax trees, a network of objects, some compiled bytecode or flying unicorn ponies who drop candy. Nevertheless, they are still read from text. If you want to change the state of REPL until you forget what it has now or it changes unbeknowest to you (“Parallelism!!!!”) it’s an option. But you can still write your tests, run, debug, change, run debug, etc. Hopefully with automated tests for extra bonus points in Software Management sainthood.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="dedicated_syntax_for_re" class="title">Regexps and other important elements have dedicated syntax</h3></div></div></div><p>But don’t worry - it’s in the “re” module and it uses read-macros/ char-macros / text macros. With the help of such macros one can even create a parser for Ruby-style syntax (or Perl’s - <code class="literal">;-)</code>), but it’ll be actively discouraged.</p><p>So for example:</p><pre class="screen">$ spark -pe '(~ ^LINE (re.s {ba([zro])(\s+)mozart} ma$1$2bozart))'</pre><p>And it will replace the first <code class="literal">baz[whitespace]mozart</code> with <code class="literal">maz[whitespace]bozart</code> , etc. The <code class="literal">\~</code> operator is similar to Perl 5’s <code class="literal">=\~</code> or perl-5.10.0’s or Perl 6’s <code class="literal">~~</code> or in that it does a smart matching of a datum (which could be a list) to an abstract operation.</p><p>We can also use other delimiters instead of <code class="literal">{…​}</code> in the <code class="literal">(re.s…​)</code> read macro:</p><pre class="screen">$ spark -pe '(~ ^LINE (re.s /ba([zro])(\s+)mozart/ ma$1$2bozart))'</pre></section><section class="section"><div class="titlepage"><div><div><h3 id="c_conventions" class="title">C/Perl/etc. conventions</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Spark will be case-sensitive</li><li class="listitem">Unicode (UTF-8) aware-and-safe</li><li class="listitem">With C-style escapes - backslash does the right thing.</li></ol></div></section><section class="section"><div class="titlepage"><div><div><h3 id="proliferation_implementations" class="title">Spark will not encourage a proliferation of implementations</h3></div></div></div><p>As you all know Lisp is a family of languages, which includes Lisp, Scheme Arc, Emacs Lisp, etc. and some people may say also Dylan, and that Perl 5, Perl 6, Ruby, Python and most other modern languages have many Lispisms in them up to being able to translate many programs written in Lisp to them line by line. (<a class="link" href="http://www.paulgraham.com/icad.html">See Paul Graham Revenge of the Nerds for the inspiration</a>)</p><p>However, some people on <a class="link" href="irc://irc.freenode.net/#scheme">#scheme</a> told me that Scheme, due to the proliferation of incompatible implementations, was not one Lisp dialect, but a family of languages called “Scheme” with a common denominator. While I’m all for <a class="link" href="http://en.wikipedia.org/wiki/Germanic_languages">Germanic languages</a> being a major sub-division of <a class="link" href="http://en.wikipedia.org/wiki/Indo-European_languages">Indo European languages</a>, also with several mutually incomprehensible languages, I’m not sure I want a “Scheme programming language”-family within Lisp. It generally shouldn’t happen with “man-made” and computer-understood languages that are more under our control.</p><p>As a result, I’d like Spark to remain a single language with only a few implementations, possibly only one for each target virtual machine (e.g: Parrotcode, the JVM, or the .NET CLR, or a C-based interpreter). Spark will be defined and compatible even in its internals, its foreign function interface, and “standard library” (which will also have something more like CPAN is for Perl 5, where every J. Random Hacker can upload their own INI parser, under a different namespace), and core functionality.</p><p>Spark will have an open-source source code (GPL-compatible BSD-style or possibly partially Artistic 2.0 in case some of the code is derived from Parrot code), which naturally can be span-off, branches, and forked. However, none of them pose a threat to the fact that the Spark implementation will remain unified.</p><p>If someone changes Spark in incompatible ways, it may either die, or forked into a new language. This language will also be Lisp and may be Spark-like but it won’t be Spark. Perl 5 which only has one major implementation (<code class="literal">perl5</code> - currently at <code class="literal">perl-5.10.0</code>), recently span off <a class="link" href="http://metacpan.org/release/kurila">kurila</a> which is fork of perl 5 that is incompatible with it and with Perl 5, on purpose. Nevertheless, while Kurila may be considered a a language in the Perl family, it is not Perl 5 any more than Perl 4 , Perl 6 , <a class="link" href="http://sleep.dashnine.org/">Sleep</a> or whatever are. So Perl 5 has still not become a family of incompatible implementations.</p><p>Another factor that will dissuade people from creating multiple implementations of Spark is that as opposed to Scheme, creating a Spark implementation from scratch is not going to be trivial. It’s not that Spark will be needlessly complixified, but that it would be needfully complex to implement to be an expressive, feature-rich and high-quality language.</p><p>To quote Bjarne Stroustrup (the creator of C++ ) from his <a class="link" href="http://www.research.att.com/~bs/bs_faq.html#Java">FAQ question about Java</a></p><div class="blockquote"><table style="border:0" class="blockquote"><tr><td style="width:10%;vertical-align:top"> </td><td style="width:80%;vertical-align:top"><p>Much of the relative simplicity of Java is - like for most new languages - partly an illusion and partly a function of its incompleteness. As time passes, Java will grow significantly in size and complexity. It will double or triple in size and grow implementation-dependent extensions or libraries. That is the way every commercially successful language has developed. Just look at any language you consider successful on a large scale. I know of no exceptions, and there are good reasons for this phenomenon. [I wrote this before 2000; now see a preview of Java 1.5.]</p></td><td style="width:10%;vertical-align:top"> </td></tr><tr><td style="width:10%;vertical-align:top"> </td><td style="text-align:right;vertical-align:top" colspan="2">--<span class="attribution"> Bjarne Stroustrup <em class="citetitle">FAQ Question about Java</em></span></td></tr></table></div><p>(I should note that, like many other FOSS hackers, I normally prefer C over C++ , and am not a big fan of C++ for most stuff. However, Stroustrup is a wickedly smart guy, and despite whatever faults his language may have, he speaks straight, and what he says here seems to make a lot of sense).</p><p>Spark hopefully won’t be as complex as C++ is today from the beginning, but will also be more complex than Scheme to allow for better expression and faster development. It also doesn’t aim to be an incremental improvement over Scheme (or Common Lisp) which seems to be the case for Arc and Clojure, but rather something like Perl 5 was to Perl 4 or Perl 6 is to Perl 5 : a paradigm shift, which Lispers and non-Lispers alike will appreciate.</p></section><section class="section"><div class="titlepage"><div><div><h3 id="ultimate_lisp" class="title">The first version of Spark will not be the ultimate Lisp</h3></div></div></div><p>Some features of Common Lisp or other Lisps will be absent in Spark, some things will be harder to do than Common Lisp or even other Lisps or other non-Lisp programming languages, and some things will not work as expected at first (bugs, etc.). A lot of it will be caused due to the fact that the primary author of this document does not consider himself a Scheme expert (and is very far from being a Common Lisp expert) and just likes Lisp, Perl 5 and other languages enough to want to promote them.</p><p>As a result, some estoric features of the popular Lisp languages today or some languages that he has not fully investigated yet, won’t be available at first. This is expected given his ignorance, enthusiasm and anxiety to get something out of the door first.</p><p>While he would still be interested in learning about whatever core library or meta-programmatic features other languages have that may prove useful for the core Spark language (or alternatively cool APIs that you think should be ported to Spark). But he has little patience to learn entire languages “fully” (if learning any non-trivial language fully is indeed possible) before starting to work on Spark. And often ignorance is a virtue.</p><p>So the first versions of Spark will still have some room for improvement. Most of it may hopefully be solvable using some meta-syntactic or meta-programming user-land libraries (as is often the case for Lisps and other dynamic languages). As for the rest, we could consider them bad design decisions that still add to the language’s colour and make it a bit more interesting to program in than a 100% perfect language. Sometimes perfection is in imperfection.</p></section></section><footer><div class="navfooter"><hr/><table style="width:100%"><tr><td style="width:40%;text-align:left"><a accesskey="p" href="index.xhtml">Prev</a> </td><td style="width:20%;text-align:center"> </td><td style="width:40%;text-align:right"> <a accesskey="n" href="why_lisp_unpopular.xhtml">Next</a></td></tr><tr><td style="width:40%;text-align:left;vertical-align:top">Spark - Pre-Birth of a Modern Lisp </td><td style="width:20%;text-align:center"><a accesskey="h" href="index.xhtml">Home</a></td><td style="width:40%;text-align:right;vertical-align:top"> Why Lisp Has Been Unpopular</td></tr></table></div></footer></body></html>